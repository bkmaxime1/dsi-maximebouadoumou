{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "cc166dbc-d723-4076-8dd8-a290d911dc9b"
   },
   "source": [
    "# Project 4: Web Scraping Indeed.com & Predicting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "59b0deac-55d6-4908-8dee-ce68611486f0"
   },
   "source": [
    "In Project 4, we practice two major skills: collecting data via  web scraping and building a binary predictor with Logistic Regression.\n",
    "\n",
    "We will collect salary information on data science jobs in a variety of markets. Using location, title, and job summary, we'll predict the salary of the job. For job posting sites, this is extraordinarily useful. While most listings DO NOT come with salary information (as you will see in this exercise), extrapolating expected salary can help guide negotiations.\n",
    "\n",
    "Normally, we can use regression for this task; however, we will convert this problem into classification and use Logistic Regression.\n",
    "\n",
    "- Q: Why would we want this to be a classification problem?\n",
    "- A: While more precision may be better, there is a fair amount of natural variance in job salaries - predicting a range be may be useful.\n",
    "\n",
    "Section one focuses on scraping Indeed.com; then we use listings with salary information to build a model and predict additional salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "1321e3c4-2105-428e-9b1b-6d958453ef1d"
   },
   "source": [
    "### Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9d959074-bf26-4000-b0da-11273e253776"
   },
   "source": [
    "Scrape job listings from Indeed.com using BeautifulSoup. Luckily, Indeed.com is a simple text page where we can easily find relevant entries. First, look at the source of an Indeed.com page: (http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\")\n",
    "\n",
    "Notice, each job listing is underneath a `div` tag with a class name of `result`. We can use BeautifulSoup to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "d9f7b5d1-b227-4bda-a87b-1606b62fb60b"
   },
   "source": [
    "#### Setup a request (using `requests`) to the URL below. Use BeautifulSoup to parse the page and extract all results (HINT: Look for div tags with class name result)\n",
    "\n",
    "The URL here has many query parameters\n",
    "\n",
    "- `q` for the job search\n",
    "- This is followed by \"+20,000\" to return results with salaries (or expected salaries >$20,000)\n",
    "- `l` for a location \n",
    "- `start` for what result number to start on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "911505d6-159f-4146-967d-a8482fe27e3d"
   },
   "outputs": [],
   "source": [
    "# URL = 'http://www.indeed.com/jobs?q=data+scientist+$20,000&l=New+York&start=10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "78446809-fa02-48df-b60f-cbeda175a498"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import bs4\n",
    "# from bs4 import BeautifulSoup\n",
    "# import urllib2\n",
    "# indeed ='http://www.indeed.com/jobs?q=data+scientist+$20,000&l=New+York&start=10'\n",
    "# page =urllib2.urlopen(indeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(page)\n",
    "# soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print page title\n",
    "# soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "c8846f3e-42a5-4714-9784-fb5d6a28524b"
   },
   "outputs": [],
   "source": [
    "# # read site in soup\n",
    "# r = requests.get(indeed)\n",
    "# soup = BeautifulSoup(r.content, \"lxml\")\n",
    "# #soup = BeautifulSoup(r.content)\n",
    "# # Append to the full set of results\n",
    "# results = soup.findAll('div', { \"class\" : \"result\" })\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "963bb376-7746-43ce-98ec-ea4162f7ead6"
   },
   "source": [
    "Let's look at one result more closely. A single `result` looks like\n",
    "\n",
    "```\n",
    "<div class=\" row result\" data-jk=\"2480d203f7e97210\" data-tn-component=\"organicJob\" id=\"p_2480d203f7e97210\" itemscope=\"\" itemtype=\"http://schema.org/JobPosting\">\n",
    "<h2 class=\"jobtitle\" id=\"jl_2480d203f7e97210\">\n",
    "<a class=\"turnstileLink\" data-tn-element=\"jobTitle\" onmousedown=\"return rclk(this,jobmap[0],1);\" rel=\"nofollow\" target=\"_blank\" title=\"AVP/Quantitative Analyst\">AVP/Quantitative Analyst</a>\n",
    "</h2>\n",
    "<span class=\"company\" itemprop=\"hiringOrganization\" itemtype=\"http://schema.org/Organization\">\n",
    "<span itemprop=\"name\">\n",
    "<a href=\"/cmp/Alliancebernstein?from=SERP&amp;campaignid=serp-linkcompanyname&amp;fromjk=2480d203f7e97210&amp;jcid=b374f2a780e04789\" target=\"_blank\">\n",
    "    AllianceBernstein</a></span>\n",
    "</span>\n",
    "<tr>\n",
    "<td class=\"snip\">\n",
    "<nobr>$117,500 - $127,500 a year</nobr>\n",
    "<div>\n",
    "<span class=\"summary\" itemprop=\"description\">\n",
    "C onduct quantitative and statistical research as well as portfolio management for various investment portfolios. Collaborate with Quantitative Analysts and</span>\n",
    "</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "```\n",
    "\n",
    "While this has some of the more verbose elements removed, we can see that there is some structure to the above:\n",
    "- The salary is available in a `nobr` element inside of a `td` element with `class='snip`.\n",
    "- The title of a job is in a link with class set to `jobtitle` and a `data-tn-element=\"jobTitle`.  \n",
    "- The location is set in a `span` with `class='location'`. \n",
    "- The company is set in a `span` with `class='company'`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "27b6ffb9-b42f-4298-b07a-10e3bab030cd"
   },
   "source": [
    "### Write 4 functions to extract each item: location, company, job, and salary.\n",
    "\n",
    "example: \n",
    "```python\n",
    "def extract_location_from_result(result):\n",
    "    return result.find ...\n",
    "```\n",
    "\n",
    "- Make sure these functions are robust and can handle cases where the data/field may not be available\n",
    "- Test the functions on the results above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "f4b0755f-42e1-438f-89fc-131a60b781cd"
   },
   "outputs": [],
   "source": [
    "# # get text\n",
    "# def extract_text(el):\n",
    "#     if el:\n",
    "#         return el.text.strip()\n",
    "#     else:\n",
    "#         return ''\n",
    "        \n",
    "# # company\n",
    "# def get_company_from_result(result):\n",
    "#     return extract_text(result.find('span', {'class' : 'company'}))\n",
    "\n",
    "# # location\n",
    "# def get_location_from_result(result):\n",
    "#     return extract_text(result.find('span', {'class' : 'location'}))\n",
    "# # summary\n",
    "# def get_summary_from_result(result):\n",
    "#     return extract_text(result.find('span', {'class' : 'summary'}))\n",
    "# # title\n",
    "# # def get_title_from_result(result):\n",
    "# #     return extract_text(result.find('span', {'class' : 'title'}))\n",
    "# def get_title_from_result(result):\n",
    "#     return extract_text(result.find('a', {'data-tn-element' : 'jobTitle'}))\n",
    "# # get salary if exists\n",
    "# def get_salary_from_result(result):\n",
    "#     salary_table = result.find('td', {'class' : 'snip'})\n",
    "#     if salary_table:\n",
    "#         snip = salary_table.find('nobr')\n",
    "#         if snip:\n",
    "#             return snip.text.strip()   \n",
    "#     return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "dc1d32a3-b13c-4919-8723-ce50dbc7660f"
   },
   "source": [
    "Now, to scale up our scraping, we need to accumulate more results. We can do this by examining the URL above.\n",
    "\n",
    "- \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "\n",
    "There are two query parameters here we can alter to collect more results: the `l=New+York` and the `start=10`. The first controls the location of the results (so we can try different city). The second controls where in the results to start and gives 10 results (so we can keep incrementing this by 10 to move further within the list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "27584c3f-f552-40a2-842a-0681b1fd6265"
   },
   "source": [
    "#### Complete the following code to collect results from multiple cities and start points. \n",
    "- Enter your city below to add it to the search\n",
    "- Remember to convert your salary to U.S. Dollars to match the other cities if the currency is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# url_template = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "# YOUR_CITY=['Atlanta','Alpharetta','San+Francisco','Mountain+View','Palo+Alto','Santa+Clara','New+York',\n",
    "#              'Jacksonville','Miami','Charlotte','Washington','Seattle','Austin','Houston','Denver','Baltimore','Arlington',\n",
    "#              'Charlottesville','Cincinnati','Cleveland','Boston','Cambridge']\n",
    "\n",
    "# #results = []\n",
    "# df= pd.DataFrame()\n",
    "# for city in YOUR_CITY:\n",
    "#     for i in range(0,500,10):\n",
    "#         LINK = url_template.format(city,i)\n",
    "#         r= requests.get(LINK)\n",
    "#         soup = BeautifulSoup(r.content)\n",
    "#         results = soup.find_all(\"div\", {\"class\" : \"result\"})\n",
    "    \n",
    "#         for result in results:\n",
    "#             if result:\n",
    "#                 company=get_company_from_result(result)\n",
    "#                 title=get_title_from_result(result)\n",
    "#                 location=get_location_from_result(result)\n",
    "#                 salary=get_salary_from_result(result)\n",
    "            \n",
    "\n",
    "#                 df= df.append({'Company Name':company, \"Job Title\":title,'Location':location, 'Salary':salary}, ignore_index =True)\n",
    "# df\n",
    "\n",
    "# df= pd.DataFrame()\n",
    "# for city in YOUR_CITY:\n",
    "#     for i in range(0,500,10):\n",
    "#         LINK = url_template.format(city,i)\n",
    "#         r= requests.get(LINK)\n",
    "#         soup = BeautifulSoup(r.content)\n",
    "#         data = soup.find_all(\"div\", {\"class\" : \"result\"})\n",
    "    \n",
    "#         for element in range(len(data)):\n",
    "#             Name = data[element].find(\"span\", {\"class\": 'company'}).get_text().strip()\n",
    "#             #if Name != None:\n",
    "#                 #Name2 =Name.get_text().strip()\n",
    "#             City = data[element].find(\"span\", {\"class\": 'location'}).get_text().strip()\n",
    "#             #if City != None:\n",
    "#                 #City2 =City.get_text().strip()\n",
    "#             Summary = data[element].find('span', {'class' : 'summary'}).get_text().strip()\n",
    "            \n",
    "#             JobTitle= data[element].find('a', {'data-tn-element' : 'jobTitle'}).get_text().strip()\n",
    "    \n",
    "        \n",
    "#             Salary = data[element].find(\"td\", {\"class\": 'snip'}).get_text().strip()\n",
    "            \n",
    "\n",
    "#             df= df.append({'Name':Name, \"City\":City,'Summary':Summary,\"JobTitle\":JobTitle, 'Salary':Salary}, ignore_index =True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df2=df.drop_duplicates().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create a dataframe and replace no entry with none for  salary no bolean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_no_salary=df2[df['Salary'].isnull()].copy()\n",
    "# df_no_salary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drop the none values\n",
    "# df2.dropna(inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df2.to_csv('indeedsoup_file.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorical = df.dtypes[df.dtypes == \"object\"].index\n",
    "# df[categorical].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "10eb5902-4727-4947-a167-2531aa12a427"
   },
   "source": [
    "#### Use the functions you wrote above to parse out the 4 fields - location, title, company and salary. Create a dataframe from the results with those 4 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALREADY BEEN DONE(SEE ABOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "d601ff2f-fbdf-4c4f-8bbe-10c4a3132cc8"
   },
   "outputs": [],
   "source": [
    "# # combine data into dictionaries\n",
    "# rows = []\n",
    "# for result in results:\n",
    "#     if result:\n",
    "#         row = {}\n",
    "\n",
    "# # create dataframe\n",
    "# import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "faac26dc-392a-4f90-a397-144a070702cb"
   },
   "source": [
    "Lastly, we need to clean up salary data. \n",
    "\n",
    "1. Only a small number of the scraped results have salary information - only these will be used for modeling.\n",
    "1. Some of the salaries are not yearly but hourly or weekly, these will not be useful to us for now\n",
    "1. Some of the entries may be duplicated\n",
    "1. The salaries are given as text and usually with ranges.\n",
    "\n",
    "#### Find the entries with annual salary entries, by filtering the entries without salaries or salaries that are not yearly (filter those that refer to hour or week). Also, remove duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "58ff72c5-eef2-4a86-93ac-22f84ed9b752"
   },
   "outputs": [],
   "source": [
    "# Filter to only the rows that have salary entries\n",
    "\n",
    "# Remove duplicates\n",
    "\n",
    "# # Filter out salary entries referring to week, hour or month\n",
    "# df2 = df2[~(df2.Salary.astype('str').str.contains('hr'))] # example\n",
    "# df2 = df2[~(df2.Salary.astype('str').str.contains('hour'))] \n",
    "# df2 = df2[~(df2.Salary.astype('str').str.contains('day'))] \n",
    "# df2 = df2[~(df2.Salary.astype('str').str.contains('week'))] \n",
    "# df2 = df2[~(df2.Salary.astype('str').str.contains('month'))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "e1f58de9-78a7-49c1-b1ff-145a8f983790"
   },
   "source": [
    "#### Write a function that takes a salary string and converts it to a number, averaging a salary range if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "f2eaea83-8f84-48d3-af17-037538d06601"
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import numpy as np\n",
    "# def extract_salary_average(salary_string):\n",
    "#     regex = r'\\$([0-9]+,[0-9]+)'\n",
    "#     matches = re.findall(regex, salary_string)\n",
    "#     return np.mean([float(salary.replace(',', '')) for salary in matches ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use '.map' or apply to transform salary to new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df2['average_salary'] = df2['Salary'].apply(lambda x: extract_salary_average(x))\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df2=dfcleanMonthlydata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def city_extract(word):\n",
    "    return word.split(',')[0].split('.')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def state_extract(word):\n",
    "    return word.split(',')[1].split(' ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2['city']=pd.DataFrame({'city':df2['Location'].apply(city_extract)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2['state']=pd.DataFrame({'state':df2['Location'].apply(state_extract)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2 = df2.drop('Location', 1)\n",
    "# del df2(['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df2 = df2.drop('Salary', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "5d66eb8a-a032-43f7-8e87-8f37612c2ce5"
   },
   "outputs": [],
   "source": [
    "# save scraped results as a CSV for Tableau/external viz\n",
    "# df2.to_csv('indeedsoup_file.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "6e8a5a1c-1580-4845-a9b6-482bc00c73cd"
   },
   "source": [
    "## Predicting salaries using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from patsy import dmatrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "634ab7c1-c76f-4b04-a36e-5ff3cb1f9eb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>average_salary</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>Centers for Disease Control and Prevention</td>\n",
       "      <td>Statistician (Health)</td>\n",
       "      <td>101553.5</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>Analytic Recruiting</td>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>82500.0</td>\n",
       "      <td>Alpharetta</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>Centers for Disease Control and Prevention</td>\n",
       "      <td>Behavioral Scientist</td>\n",
       "      <td>85399.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Stackfolio</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>Analytic Recruiting</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                Company Name  \\\n",
       "0          19  Centers for Disease Control and Prevention   \n",
       "1          23                         Analytic Recruiting   \n",
       "2          27  Centers for Disease Control and Prevention   \n",
       "3          54                                  Stackfolio   \n",
       "4          57                         Analytic Recruiting   \n",
       "\n",
       "               Job Title  average_salary        city state  \n",
       "0  Statistician (Health)        101553.5     Atlanta    GA  \n",
       "1  Junior Data Scientist         82500.0  Alpharetta    GA  \n",
       "2   Behavioral Scientist         85399.0     Atlanta    GA  \n",
       "3    Lead Data Scientist         80000.0     Atlanta    GA  \n",
       "4  Senior Data Scientist        112500.0     Atlanta    GA  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the the data of scraped salaries\n",
    "df3=pd.read_csv(\"indeedsoup_file.csv\")\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c3ed6de7-8fe0-4cf4-abbd-abb2a188e05b"
   },
   "source": [
    "#### We want to predict a binary variable - whether the salary was low or high. Compute the median salary and create a new binary variable that is true when the salary is high (above the median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "073e3f3e-21bc-4ab7-ae2e-272be0a409cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97500.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate median and create feature with 1 as high salary\n",
    "df3['average_salary'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3['high_salary']= [1 if x > df3['average_salary'].median() else 0 for x in df3['average_salary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>average_salary</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>high_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>Centers for Disease Control and Prevention</td>\n",
       "      <td>Statistician (Health)</td>\n",
       "      <td>101553.5</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>Analytic Recruiting</td>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>82500.0</td>\n",
       "      <td>Alpharetta</td>\n",
       "      <td>GA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>Centers for Disease Control and Prevention</td>\n",
       "      <td>Behavioral Scientist</td>\n",
       "      <td>85399.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Stackfolio</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>Analytic Recruiting</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                Company Name  \\\n",
       "0          19  Centers for Disease Control and Prevention   \n",
       "1          23                         Analytic Recruiting   \n",
       "2          27  Centers for Disease Control and Prevention   \n",
       "3          54                                  Stackfolio   \n",
       "4          57                         Analytic Recruiting   \n",
       "\n",
       "               Job Title  average_salary        city state  high_salary  \n",
       "0  Statistician (Health)        101553.5     Atlanta    GA            1  \n",
       "1  Junior Data Scientist         82500.0  Alpharetta    GA            0  \n",
       "2   Behavioral Scientist         85399.0     Atlanta    GA            0  \n",
       "3    Lead Data Scientist         80000.0     Atlanta    GA            0  \n",
       "4  Senior Data Scientist        112500.0     Atlanta    GA            1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.536198\n",
      "         Iterations: 12\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 24\n",
      "         Hessian evaluations: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import patsy\n",
    "y,X =patsy.dmatrices('high_salary~state',data=df3)\n",
    "\n",
    "logit=sm.Logit(y,X)\n",
    "\n",
    "#fit the model\n",
    "result=logit.fit(method='ncg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>high_salary</td>   <th>  No. Observations:  </th>  <td>   241</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   227</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    13</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 22 Nov 2016</td> <th>  Pseudo R-squ.:     </th>  <td>0.2260</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>08:28:02</td>     <th>  Log-Likelihood:    </th> <td> -129.22</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -166.95</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>7.862e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>   <td>    1.7048</td> <td>    0.544</td> <td>    3.136</td> <td> 0.002</td> <td>    0.639     2.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.CO]</th> <td>   -2.6210</td> <td>    0.803</td> <td>   -3.262</td> <td> 0.001</td> <td>   -4.196    -1.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.DC]</th> <td>   -0.8575</td> <td>    0.730</td> <td>   -1.174</td> <td> 0.240</td> <td>   -2.289     0.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.FL]</th> <td>   -3.5765</td> <td>    0.934</td> <td>   -3.829</td> <td> 0.000</td> <td>   -5.407    -1.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.GA]</th> <td>   -2.7164</td> <td>    0.798</td> <td>   -3.405</td> <td> 0.001</td> <td>   -4.280    -1.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.MA]</th> <td>   -1.0761</td> <td>    0.698</td> <td>   -1.542</td> <td> 0.123</td> <td>   -2.444     0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.MD]</th> <td>   -4.3074</td> <td>    0.912</td> <td>   -4.721</td> <td> 0.000</td> <td>   -6.096    -2.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.NC]</th> <td>   10.0949</td> <td>  149.003</td> <td>    0.068</td> <td> 0.946</td> <td> -281.946   302.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.NJ]</th> <td>   -2.3979</td> <td>    1.340</td> <td>   -1.790</td> <td> 0.074</td> <td>   -5.024     0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.NY]</th> <td>   -1.1100</td> <td>    0.626</td> <td>   -1.772</td> <td> 0.076</td> <td>   -2.338     0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.OH]</th> <td>   -1.9924</td> <td>    0.937</td> <td>   -2.125</td> <td> 0.034</td> <td>   -3.830    -0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.TX]</th> <td>   -2.3979</td> <td>    0.739</td> <td>   -3.247</td> <td> 0.001</td> <td>   -3.845    -0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.VA]</th> <td>   -2.3979</td> <td>    0.892</td> <td>   -2.689</td> <td> 0.007</td> <td>   -4.146    -0.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.WA]</th> <td>   -1.5224</td> <td>    0.814</td> <td>   -1.871</td> <td> 0.061</td> <td>   -3.117     0.072</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:            high_salary   No. Observations:                  241\n",
       "Model:                          Logit   Df Residuals:                      227\n",
       "Method:                           MLE   Df Model:                           13\n",
       "Date:                Tue, 22 Nov 2016   Pseudo R-squ.:                  0.2260\n",
       "Time:                        08:28:02   Log-Likelihood:                -129.22\n",
       "converged:                       True   LL-Null:                       -166.95\n",
       "                                        LLR p-value:                 7.862e-11\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "-------------------------------------------------------------------------------\n",
       "Intercept       1.7048      0.544      3.136      0.002         0.639     2.770\n",
       "state[T.CO]    -2.6210      0.803     -3.262      0.001        -4.196    -1.046\n",
       "state[T.DC]    -0.8575      0.730     -1.174      0.240        -2.289     0.574\n",
       "state[T.FL]    -3.5765      0.934     -3.829      0.000        -5.407    -1.746\n",
       "state[T.GA]    -2.7164      0.798     -3.405      0.001        -4.280    -1.153\n",
       "state[T.MA]    -1.0761      0.698     -1.542      0.123        -2.444     0.292\n",
       "state[T.MD]    -4.3074      0.912     -4.721      0.000        -6.096    -2.519\n",
       "state[T.NC]    10.0949    149.003      0.068      0.946      -281.946   302.136\n",
       "state[T.NJ]    -2.3979      1.340     -1.790      0.074        -5.024     0.228\n",
       "state[T.NY]    -1.1100      0.626     -1.772      0.076        -2.338     0.118\n",
       "state[T.OH]    -1.9924      0.937     -2.125      0.034        -3.830    -0.155\n",
       "state[T.TX]    -2.3979      0.739     -3.247      0.001        -3.845    -0.950\n",
       "state[T.VA]    -2.3979      0.892     -2.689      0.007        -4.146    -0.650\n",
       "state[T.WA]    -1.5224      0.814     -1.871      0.061        -3.117     0.072\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "3c7ec3d2-87a0-4290-9d83-a6f4a9ae7e9c"
   },
   "source": [
    "### Q: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "987666b2-d8e6-4715-b499-c9d314fb70ce"
   },
   "source": [
    "It is 50% if we guess randomly, half the salaries will be below the median and half will be above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ea7e00cb-9956-44ec-b585-7b95f4d6284c"
   },
   "source": [
    "#### Create a Logistic Regression model to predict High/Low salary using statsmodel. Start by ONLY using the location as a feature. Display the coefficients and write a short summary of what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "ce9161b3-eff3-475c-a087-a2be38d7f626"
   },
   "outputs": [],
   "source": [
    "# create statsmodel and summary\n",
    "import statsmodels.formula.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.365191\n",
      "         Iterations: 13\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 26\n",
      "         Hessian evaluations: 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>high_salary</td>   <th>  No. Observations:  </th>  <td>   241</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   227</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    13</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 22 Nov 2016</td> <th>  Pseudo R-squ.:     </th>  <td>0.2260</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>09:03:19</td>     <th>  Log-Likelihood:    </th> <td> -129.22</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -166.95</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>7.862e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>   <td>    1.7048</td> <td>    0.544</td> <td>    3.136</td> <td> 0.002</td> <td>    0.639     2.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.CO]</th> <td>   -2.6210</td> <td>    0.803</td> <td>   -3.262</td> <td> 0.001</td> <td>   -4.196    -1.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.DC]</th> <td>   -0.8575</td> <td>    0.730</td> <td>   -1.174</td> <td> 0.240</td> <td>   -2.289     0.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.FL]</th> <td>   -3.5765</td> <td>    0.934</td> <td>   -3.829</td> <td> 0.000</td> <td>   -5.407    -1.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.GA]</th> <td>   -2.7164</td> <td>    0.798</td> <td>   -3.405</td> <td> 0.001</td> <td>   -4.280    -1.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.MA]</th> <td>   -1.0761</td> <td>    0.698</td> <td>   -1.542</td> <td> 0.123</td> <td>   -2.444     0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.MD]</th> <td>   -4.3074</td> <td>    0.912</td> <td>   -4.721</td> <td> 0.000</td> <td>   -6.096    -2.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.NC]</th> <td>   10.0949</td> <td>  149.003</td> <td>    0.068</td> <td> 0.946</td> <td> -281.946   302.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.NJ]</th> <td>   -2.3979</td> <td>    1.340</td> <td>   -1.790</td> <td> 0.074</td> <td>   -5.024     0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.NY]</th> <td>   -1.1100</td> <td>    0.626</td> <td>   -1.772</td> <td> 0.076</td> <td>   -2.338     0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.OH]</th> <td>   -1.9924</td> <td>    0.937</td> <td>   -2.125</td> <td> 0.034</td> <td>   -3.830    -0.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.TX]</th> <td>   -2.3979</td> <td>    0.739</td> <td>   -3.247</td> <td> 0.001</td> <td>   -3.845    -0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.VA]</th> <td>   -2.3979</td> <td>    0.892</td> <td>   -2.689</td> <td> 0.007</td> <td>   -4.146    -0.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>state[T.WA]</th> <td>   -1.5224</td> <td>    0.814</td> <td>   -1.871</td> <td> 0.061</td> <td>   -3.117     0.072</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:            high_salary   No. Observations:                  241\n",
       "Model:                          Logit   Df Residuals:                      227\n",
       "Method:                           MLE   Df Model:                           13\n",
       "Date:                Tue, 22 Nov 2016   Pseudo R-squ.:                  0.2260\n",
       "Time:                        09:03:19   Log-Likelihood:                -129.22\n",
       "converged:                       True   LL-Null:                       -166.95\n",
       "                                        LLR p-value:                 7.862e-11\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "-------------------------------------------------------------------------------\n",
       "Intercept       1.7048      0.544      3.136      0.002         0.639     2.770\n",
       "state[T.CO]    -2.6210      0.803     -3.262      0.001        -4.196    -1.046\n",
       "state[T.DC]    -0.8575      0.730     -1.174      0.240        -2.289     0.574\n",
       "state[T.FL]    -3.5765      0.934     -3.829      0.000        -5.407    -1.746\n",
       "state[T.GA]    -2.7164      0.798     -3.405      0.001        -4.280    -1.153\n",
       "state[T.MA]    -1.0761      0.698     -1.542      0.123        -2.444     0.292\n",
       "state[T.MD]    -4.3074      0.912     -4.721      0.000        -6.096    -2.519\n",
       "state[T.NC]    10.0949    149.003      0.068      0.946      -281.946   302.136\n",
       "state[T.NJ]    -2.3979      1.340     -1.790      0.074        -5.024     0.228\n",
       "state[T.NY]    -1.1100      0.626     -1.772      0.076        -2.338     0.118\n",
       "state[T.OH]    -1.9924      0.937     -2.125      0.034        -3.830    -0.155\n",
       "state[T.TX]    -2.3979      0.739     -3.247      0.001        -3.845    -0.950\n",
       "state[T.VA]    -2.3979      0.892     -2.689      0.007        -4.146    -0.650\n",
       "state[T.WA]    -1.5224      0.814     -1.871      0.061        -3.117     0.072\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = sm.logit('high_salary~ state + city', data=df3).fit(method='ncg')\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "1ecd7811-d200-44bc-942f-4beb76d2689c"
   },
   "source": [
    "#### Create a few new variables in your dataframe to represent interesting features of a job title.\n",
    "- For example, create a feature that represents whether 'Senior' or 'Manager' is in the title \n",
    "- Then build a new Logistic Regression model with these features. Do they add any value? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "b847f46e-1626-4340-86fb-08dea8c31a84"
   },
   "outputs": [],
   "source": [
    "# create senior, director, and manager dummies\n",
    "df3['is_junior'] = df3['Job Title'].str.contains('Junior').astype(int)\n",
    "df3['is_senior'] = df3['Job Title'].str.contains('Senior').astype(int)\n",
    "df3['is_lead'] = df3['Job Title'].str.contains('Lead').astype(int)\n",
    "df3['is_director'] = df3['Job Title'].str.contains('Director').astype(int)\n",
    "df3['is_manager'] = df3['Job Title'].str.contains('Manager').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "7ca5cfdd-958c-4199-aafa-3d3f6c5ba3c4"
   },
   "source": [
    "#### Rebuild this model with scikit-learn.\n",
    "- You can either create the dummy features manually or use the `dmatrix` function from `patsy`\n",
    "- Remember to scale the feature variables as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale, (patsy optional), and fit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from patsy import dmatrix\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model = LogisticRegression(penalty = 'l2', C=0.1 , solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: MA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-92471287f2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# xs = preprocessing.scale(df3[\"state\",\"city\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"high_salary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    127\u001b[0m     X = check_array(X, accept_sparse='csr', copy=copy, ensure_2d=False,\n\u001b[1;32m    128\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the scale function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    371\u001b[0m                                       force_all_finite)\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: MA"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "# xs = preprocessing.scale(df3[\"state\",\"city\"])\n",
    "xs = preprocessing.scale(df3[\"state\"])\n",
    "ys = preprocessing.scale(df3[\"high_salary\"])\n",
    "\n",
    "plt.scatter(xs, ys, color='r')\n",
    "plt.xlabel(\"NOX standardized\")\n",
    "plt.ylabel(\"TAX standardized\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: CA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-122fdd180966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64, \n\u001b[0;32m-> 1142\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    508\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    509\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    371\u001b[0m                                       force_all_finite)\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: CA"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=30)\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfTest2=X_test\n",
    "dfTest2['predictedSalary'] = model2.predict(X_test)\n",
    "dfTest2[\"actualSalary\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print metrics.accuracy_score(y_test, dfTest2['predictedSalary'])\n",
    "print pd.crosstab(y_test, dfTest2['predictedSalary'], rownames=['Actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build confusion matrices for the models above\n",
    "Y_1_pred = logreg_1.predict(X_train)\n",
    "Y_2_pred = logreg_2.predict(X_train)\n",
    "\n",
    "conmat_1 = confusion_matrix(Y_train, Y_1_pred, labels=logreg_1.classes_)\n",
    "conmat_1 = pd.DataFrame(conmat_1, columns=logreg_1.classes_, index=logreg_1.classes_)\n",
    "\n",
    "conmat_2 = confusion_matrix(Y_train, Y_2_pred, labels=logreg_2.classes_)\n",
    "conmat_2 = pd.DataFrame(conmat_2, columns=logreg_2.classes_, index=logreg_2.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "1e6c6902-2b4a-49f0-b4c7-935a26577d22"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy, AUC, precision and recall of the model. \n",
    "- Discuss the differences and explain when you want a high-recall or a high-precision model in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X, y, scoring='accuracy', cv=10)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "3667427c-6534-4dcd-8770-f492b0e3a39e"
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'roc_auc']: # example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "Y_score = logreg.decision_function(X_test)\n",
    "\n",
    "FPR = dict()\n",
    "TPR = dict()\n",
    "ROC_AUC = dict()\n",
    "\n",
    "# For class 1, find the area under the curve\n",
    "FPR[1], TPR[1], _ = roc_curve(Y_test, Y_score)\n",
    "ROC_AUC[1] = auc(FPR[1], TPR[1])\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for cancer detection', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the intuition for the ROC curve?\n",
    "\n",
    "As the class assignment threshold increases for the positive class (has cancer), the false positive rate and true positive rate necessarily increase. For a classifier performing at chance, this would be the diagonal dotted line: an equal chance of false positives and true positives. \n",
    "\n",
    "The greater the area under the curve, the higher the ratio of true positives to false positives as the threshold becomes more lenient. Thus, the greater the area under the curve, the higher the quality of the classification model. In the Wisconsin breast cancer data the area under the curve is 0.99, indicating a nearly perfect model. Most classification problems will never get close to this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into 66% training set and 33% testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#metrics_pct = np.array(bcw.metrics_pct.values)\n",
    "#metrics_pct = metrics_pct[:, np.newaxis]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_df, target_df, test_size=0.33, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the logistic regression on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=5)\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# the input format is almost always (test, predict), but always check with documentation!\n",
    "conmat = np.array(confusion_matrix(Y_test, Y_pred))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['is_healthy', 'has_cancer'],\n",
    "                         columns=['predicted_healthy', 'predicted_cancer'])\n",
    "\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4263b1c0-bfde-42bf-ab45-71c7cd798835"
   },
   "source": [
    "### Compare L1 and L2 regularization for this logistic regression model. What effect does this have on the coefficients learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=30)\n",
    "L1Model= LogisticRegression(penalty='l1')\n",
    "L1Model.fit(X_train, y_train)\n",
    "dfTest3=X_test\n",
    "dfTest3['predictedSalary'] = L1Model.predict(X_test)\n",
    "dfTest3[\"actualSalary\"] = y_test\n",
    "print metrics.accuracy_score(y_test, dfTest3['predictedSalary'])\n",
    "print pd.crosstab(y_test, dfTest3['predictedSalary'], rownames=['Actual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=30)\n",
    "L2Model= LogisticRegression(penalty='l2')\n",
    "L2Model.fit(X_train, y_train)\n",
    "dfTest4=X_test\n",
    "dfTest4['predictedSalary'] = L2Model.predict(X_test)\n",
    "dfTest4[\"actualSalary\"] = y_test\n",
    "print metrics.accuracy_score(y_test, dfTest4['predictedSalary'])\n",
    "print pd.crosstab(y_test, dfTest4['predictedSalary'], rownames=['Actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "2e7d6a29-a515-468a-9953-9d73a0f81de0"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty = 'l1', C=1.0)\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'roc_auc']:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "32d908a3-89d2-474c-a7f0-199bfae6da7e"
   },
   "outputs": [],
   "source": [
    "model.fit(X_scaled, y)\n",
    "\n",
    "df = pd.DataFrame({'features' : X.design_info.column_names, 'coef': model.coef_[0,:]})\n",
    "df.sort_values('coef', ascending=False, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": true,
    "id": "82f16f60-6c8b-4376-b3ec-b8ec61a0cde7"
   },
   "source": [
    "#### Optional: Continue to incorporate other text features from the title or summary that you believe will predict the salary and examine their coefficients. Take ~100 scraped entries with salaries. Convert them to use with your model and predict the salary. Which entries have the highest predicted salaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "3f242a55-4518-4c95-ae90-6888c68077d3"
   },
   "source": [
    "# Bonus Section: Use Count Vectorizer from scikit-learn to create features from the text summaries. \n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate the logistic regression model using these. Does this improve the model performance? \n",
    "- What text features are most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "757205dc-443d-4754-9d23-e591e0921c02"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform()\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'roc_auc']:\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=3, scoring=metric)\n",
    "    print(metric, scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "f44df3c1-cf82-4271-8660-fdd0052097b6"
   },
   "outputs": [],
   "source": [
    "model.fit(X_scaled, y)\n",
    "\n",
    "df = pd.DataFrame({'features' : vectorizer.get_feature_names(), 'coef': model.coef_[0,:]})\n",
    "df.sort_values('coef', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "e182bbe4-2a72-4e75-a3e8-c117688cb8a6"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "a15ef8ea-3130-4c08-a165-ac34d2a8d829"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "focus": false,
    "id": "b8a13337-0cde-4117-a928-ffae14661453"
   },
   "outputs": [],
   "source": [
    "# retest L1 and L2 regularization\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model = LogisticRegressionCV()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "focus": false,
    "id": "936cd752-6b3f-450f-bfb6-1659c6e71539"
   },
   "source": [
    "Score: | /24\n",
    "------|-------\n",
    "Identify: Problem Statement and Hypothesis | \n",
    "Acquire: Import Data using BeautifulSoup| \n",
    "Parse: Clean and Organize Data| \n",
    "Model: Perform Logistic Regression| \n",
    "Evaluate: Logistic Regression Results\t|\n",
    "Present: Blog Report with Findings and Recommendations\t\t| \n",
    "Interactive Tableau visualizations | \n",
    "Regularization |\n",
    "Bonus: Countvectorizer  | "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
