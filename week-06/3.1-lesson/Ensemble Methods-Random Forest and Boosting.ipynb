{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices, build_design_matrices\n",
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will encode the features using a One Hot encoding scheme, i.e. we will consider them as categorical variables. We also need to encode the label using the LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(df['acceptability'])\n",
    "X = pd.get_dummies(df.drop('acceptability', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to compare the performance of the following 4 algorithms:\n",
    "Decision Trees\n",
    "Bagging + Decision Trees\n",
    "Random Forest\n",
    "Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order for our results to be consistent we have to expose the models to exactly the same Cross Validation scheme. Let's start by initializing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # patsy our design matrix\n",
    "# from patsy import dmatrices, build_design_matrices\n",
    "# import patsy\n",
    "# y,X =patsy.dmatrices('acceptability ~C(safety) + C(lug_boot ) + C(maint) +C(buying) +doors +persons' ,data = df, return_type = 'dataframe')\n",
    "# X\n",
    "# # >>> from sklearn.model_selection import StratifiedKFold\n",
    "# # >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "# # >>> y = np.array([0, 0, 1, 1])\n",
    "# # >>> skf = StratifiedKFold(n_splits=2)\n",
    "# cv = StratifiedKFold(n_folds=3, shuffle=True, random_state=4)\n",
    "# cv.get_n_splits(X, y)\n",
    "# print(cv)\n",
    "# # StratifiedKFold(n_splits=2, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "cv = StratifiedKFold(y, n_folds=3, shuffle=True, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize a Decision Tree Classifier and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Score:\t0.966 ± 0.009\n",
      "Bagging DT Score:\t0.968 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "bdt = BaggingClassifier(DecisionTreeClassifier())\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3))\n",
    "score(bdt, \"Bagging DT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn now:\n",
    "\n",
    "Initialize the following models and check their performance:\n",
    "\n",
    "Bagging + Decision Trees\n",
    "Random Forest\n",
    "Extra Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Score:\t0.966 ± 0.009\n",
      "Bagging DT Score:\t0.968 ± 0.004\n",
      "Random Forest Score:\t0.941 ± 0.008\n",
      "Extra Trees Score:\t0.957 ± 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "bdt = BaggingClassifier(DecisionTreeClassifier())\n",
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "et = ExtraTreesClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "def score(model, name):\n",
    "    s = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "    print \"{} Score:\\t{:0.3} ± {:0.3}\".format(name, s.mean().round(3), s.std().round(3))\n",
    "\n",
    "score(dt, \"Decision Tree\")\n",
    "score(bdt, \"Bagging DT\")\n",
    "score(rf, \"Random Forest\")\n",
    "score(et, \"Extra Trees\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the Bagging Decision tree seems to still be performing better than the other models, although the scores are compatible within the error. With other datasets the Random Forest and the Extra Trees model could be performing better and thus are worth testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the performance of the AdaBoost and GradientBoostingClassifier models on the car dataset. Use the code you developed above as a starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Your turn now:\n",
    "\n",
    "Initialize the following models and check their performance:\n",
    "\n",
    "Bagging + Decision Trees\n",
    "Random Forest\n",
    "Extra Trees\n",
    "\n",
    "then\n",
    "AdaBoost \n",
    "GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Score:\t0.966 ± 0.009\n",
      "Bagging DT Score:\t0.968 ± 0.004\n",
      "Random Forest Score:\t0.941 ± 0.008\n",
      "Extra Trees Score:\t0.957 ± 0.002\n",
      "AdaBoost Score:\t0.811 ± 0.002\n",
      "Gradient Boosting Classifier Score:\t0.982 ± 0.006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "ab = AdaBoostClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "bdt = BaggingClassifier(DecisionTreeClassifier())\n",
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "et = ExtraTreesClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "def score(model, name):\n",
    "    s = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "    print \"{} Score:\\t{:0.3} ± {:0.3}\".format(name, s.mean().round(3), s.std().round(3))\n",
    "\n",
    "score(dt, \"Decision Tree\")\n",
    "score(bdt, \"Bagging DT\")\n",
    "score(rf, \"Random Forest\")\n",
    "score(et, \"Extra Trees\")\n",
    "score(ab, \"AdaBoost\")\n",
    "score(gb, \"Gradient Boosting Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class we learned about Random Forest, Extremely randomized trees and Boosting. They are different ways to improve the performance of a weak learner.\n",
    "\n",
    "Some of these methods will perform better in some cases, some better in other cases. For example, Decision Trees are more nimble and easier to communicate, but have a tendency to overfit. On the other hand Ensemble methods perform better in more complex scenarios, but may become very complicated and harder to explain. Have a look here for a couple of examples from real world startup Wise.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: Can you think of what could be limitations of these methods?\n",
    "    Answer:\n",
    "\n",
    "They don't scale very well to large datasets, Boosting in particular\n",
    "They are black boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
